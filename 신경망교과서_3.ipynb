{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "thick-violation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess(df):\n",
    "    # DataFrame 내 결측값을 제거한다\n",
    "    def remove_missing_values(df):\n",
    "        df = df.dropna()\n",
    "        return df\n",
    "\n",
    "    # 요금 이상치를 제거한다\n",
    "    def remove_fare_amount_outliers(df, lower_bound, upper_bound):\n",
    "        df = df[(df['fare_amount'] > lower_bound) & (df['fare_amount'] <= upper_bound)]\n",
    "        return df\n",
    "\n",
    "    # 승객 수 이상치를 최빈값으로 대체한다\n",
    "    def replace_passenger_count_outliers(df):\n",
    "        mode = df['passenger_count'].mode().values[0]\n",
    "        df.loc[df['passenger_count'] == 0, 'passenger_count'] = 1\n",
    "        return df\n",
    "\n",
    "    # 위도 경도 이상치를 제거한다\n",
    "    def remove_lat_long_outliers(df):\n",
    "        # 뉴욕시 경도 범위\n",
    "        nyc_min_longitude = -74.05\n",
    "        nyc_max_longitude = -73.75\n",
    "\n",
    "        # 뉴욕시 위도 범위\n",
    "        nyc_min_latitude = 40.63\n",
    "        nyc_max_latitude = 40.85\n",
    "\n",
    "        # 뉴욕시 반경 내 위치만 남긴다\n",
    "        for long in ['pickup_longitude', 'dropoff_longitude']:\n",
    "          df = df[(df[long] > nyc_min_longitude) & (df[long] < nyc_max_longitude)]\n",
    "\n",
    "        for lat in ['pickup_latitude', 'dropoff_latitude']:\n",
    "          df = df[(df[lat] > nyc_min_latitude) & (df[lat] < nyc_max_latitude)]\n",
    "        return df\n",
    "\n",
    "\n",
    "    df = remove_missing_values(df)\n",
    "    df = remove_fare_amount_outliers(df, lower_bound = 0, upper_bound = 100)\n",
    "    df = replace_passenger_count_outliers(df)\n",
    "    df = remove_lat_long_outliers(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def feature_engineer(df):\n",
    "    # 연, 월, 일, 요일, 시간 칼럼을 새로 만든다\n",
    "    def create_time_features(df):\n",
    "        df['year'] = df['pickup_datetime'].dt.year\n",
    "        df['month'] = df['pickup_datetime'].dt.month\n",
    "        df['day'] = df['pickup_datetime'].dt.day\n",
    "        df['day_of_week'] = df['pickup_datetime'].dt.dayofweek\n",
    "        df['hour'] = df['pickup_datetime'].dt.hour\n",
    "        df = df.drop(['pickup_datetime'], axis=1)\n",
    "        return df\n",
    "\n",
    "    # 유클리드 거리를 계산하는 함수\n",
    "    def euc_distance(lat1, long1, lat2, long2):\n",
    "        return(((lat1-lat2)**2 + (long1-long2)**2)**0.5)\n",
    "\n",
    "    # 이동 거리 칼럼을 추가한다\n",
    "    def create_pickup_dropoff_dist_features(df):\n",
    "        df['travel_distance'] = euc_distance(df['pickup_latitude'], df['pickup_longitude'], df['dropoff_latitude'], df['dropoff_longitude'])\n",
    "        return df\n",
    "\n",
    "    # 공항과 거리 칼럼을 추가한다\n",
    "    def create_airport_dist_features(df):\n",
    "        airports = {'JFK_Airport': (-73.78,40.643),\n",
    "                    'Laguardia_Airport': (-73.87, 40.77),\n",
    "                    'Newark_Airport' : (-74.18, 40.69)}\n",
    "\n",
    "        for airport in airports:\n",
    "            df['pickup_dist_' + airport] = euc_distance(df['pickup_latitude'], df['pickup_longitude'], airports[airport][1], airports[airport][0])\n",
    "            df['dropoff_dist_' + airport] = euc_distance(df['dropoff_latitude'], df['dropoff_longitude'], airports[airport][1], airports[airport][0])\n",
    "        return df\n",
    "\n",
    "    df = create_time_features(df)\n",
    "    df = create_pickup_dropoff_dist_features(df)\n",
    "    df = create_airport_dist_features(df)\n",
    "    df = df.drop(['key'], axis=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-penny",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "correct-mailing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in the dataset. This will take some time..\n",
      "\n",
      "      Dataset not found in your computer.\n",
      "      Please follow the instructions in the link below to download the dataset:\n",
      "      https://raw.githubusercontent.com/PacktPublishing/Neural-Network-Projects-with-Python/master/chapter3/how_to_download_the_dataset.txt\n",
      "      \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b3fce750e084>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# 데이터 전처리와 특징 공학 과정을 거친다\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_engineer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# %load ./신경망교과서/chapter03/main.py\n",
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\")\n",
    "from utils import preprocess, feature_engineer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import BatchNormalization\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "try:\n",
    "    print(\"Reading in the dataset. This will take some time..\")\n",
    "    df = pd.read_csv('NYC_taxi.csv', parse_dates=['pickup_datetime'], nrows=500000)\n",
    "except:\n",
    "    print(\"\"\"\n",
    "      Dataset not found in your computer.\n",
    "      Please follow the instructions in the link below to download the dataset:\n",
    "      https://raw.githubusercontent.com/PacktPublishing/Neural-Network-Projects-with-Python/master/chapter3/how_to_download_the_dataset.txt\n",
    "      \"\"\")\n",
    "    quit()\n",
    "\n",
    "\n",
    "# 데이터 전처리와 특징 공학 과정을 거친다\n",
    "df = preprocess(df)\n",
    "df = feature_engineer(df)\n",
    "\n",
    "# 변수 스케일링 과정을 실행한다\n",
    "df_prescaled = df.copy()\n",
    "df_scaled = df.drop(['fare_amount'], axis=1)\n",
    "df_scaled = scale(df_scaled)\n",
    "cols = df.columns.tolist()\n",
    "cols.remove('fare_amount')\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=cols, index=df.index)\n",
    "df_scaled = pd.concat([df_scaled, df['fare_amount']], axis=1)\n",
    "df = df_scaled.copy()\n",
    "\n",
    "# DataFrame을 훈련 데이터셋과 테스트 데이터셋으로 나눈다\n",
    "X = df.loc[:, df.columns != 'fare_amount']\n",
    "y = df.fare_amount\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# 케라스 신경망을 구축한다\n",
    "model=Sequential()\n",
    "model.add(Dense(128, activation= 'relu', input_dim=X_train.shape[1]))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Dense(64, activation= 'relu'))\n",
    "model.add(Dense(32, activation= 'relu'))\n",
    "model.add(Dense(8, activation= 'relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=1)\n",
    "\n",
    "# 결과를 분석한다\n",
    "train_pred = model.predict(X_train)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "test_pred = model.predict(X_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "print(\"Train RMSE: {:0.2f}\".format(train_rmse))\n",
    "print(\"Test RMSE: {:0.2f}\".format(test_rmse))\n",
    "print('------------------------')\n",
    "\n",
    "def predict_random(df_prescaled, X_test, model):\n",
    "    sample = X_test.sample(n=1, random_state=np.random.randint(low=0, high=10000))\n",
    "    idx = sample.index[0]\n",
    "\n",
    "    actual_fare = df_prescaled.loc[idx,'fare_amount']\n",
    "    day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    day_of_week = day_names[df_prescaled.loc[idx,'day_of_week']]\n",
    "    hour = df_prescaled.loc[idx,'hour']\n",
    "    predicted_fare = model.predict(sample)[0][0]\n",
    "    rmse = np.sqrt(np.square(predicted_fare-actual_fare))\n",
    "\n",
    "    print(\"Trip Details: {}, {}:00hrs\".format(day_of_week, hour))\n",
    "    print(\"Actual fare: ${:0.2f}\".format(actual_fare))\n",
    "    print(\"Predicted fare: ${:0.2f}\".format(predicted_fare))\n",
    "    print(\"RMSE: ${:0.2f}\".format(rmse))\n",
    "\n",
    "predict_random(df_prescaled, X_test, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-premiere",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-forward",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
